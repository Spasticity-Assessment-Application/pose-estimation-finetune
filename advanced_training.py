"""
Entra√Ænement avanc√© avec fine-tuning progressif du backbone
Techniques avanc√©es : Mixup, CutMix, Cosine Annealing, SWA, Label Smoothing, Mixed Precision
"""
import os
import config
from train import create_callbacks, create_data_augmentation, evaluate_model, save_final_model, plot_training_history
from tensorflow import keras
from tensorflow.keras.callbacks import LearningRateScheduler, Callback
import tensorflow as tf
import numpy as np


def mixup(x, y, alpha=0.2):
    """Mixup augmentation pour heatmaps"""
    batch_size = tf.shape(x)[0]
    lam = tf.random.uniform([batch_size, 1, 1, 1], 0, alpha)
    
    # Shuffle indices
    indices = tf.random.shuffle(tf.range(batch_size))
    
    # Mix images and labels
    x_mixed = lam * x + (1 - lam) * tf.gather(x, indices)
    y_mixed = lam * y + (1 - lam) * tf.gather(y, indices)
    
    return x_mixed, y_mixed


def cutmix(x, y, alpha=1.0):
    """CutMix augmentation pour heatmaps - Version simplifi√©e et stable"""
    # Convertir en numpy pour √©viter les probl√®mes TensorFlow
    x_np = x.numpy() if hasattr(x, 'numpy') else x
    y_np = y.numpy() if hasattr(y, 'numpy') else y
    
    batch_size = x_np.shape[0]
    img_h, img_w = x_np.shape[1], x_np.shape[2]
    
    # Lambda value
    lam = np.random.beta(alpha, alpha)
    
    # Random box
    cut_ratio = np.sqrt(1.0 - lam)
    cut_h = int(img_h * cut_ratio)
    cut_w = int(img_w * cut_ratio)
    
    # Random center point
    cx = np.random.randint(0, img_w)
    cy = np.random.randint(0, img_h)
    
    # Box coordinates
    x1 = np.clip(cx - cut_w // 2, 0, img_w)
    y1 = np.clip(cy - cut_h // 2, 0, img_h)
    x2 = np.clip(cx + cut_w // 2, 0, img_w)
    y2 = np.clip(cy + cut_h // 2, 0, img_h)
    
    # Shuffle indices
    indices = np.random.permutation(batch_size)
    
    # Mix images
    x_mixed = x_np.copy()
    x_mixed[:, y1:y2, x1:x2, :] = x_np[indices, y1:y2, x1:x2, :]
    
    # Adjust lambda based on actual cut area
    lam_adjusted = 1 - ((x2 - x1) * (y2 - y1)) / (img_h * img_w)
    
    # Mix labels
    y_mixed = lam_adjusted * y_np + (1 - lam_adjusted) * y_np[indices]
    
    return x_mixed, y_mixed


class AdvancedDataGenerator(keras.utils.Sequence):
    """G√©n√©rateur avec Mixup/CutMix"""
    def __init__(self, X, y, batch_size=32, use_mixup=True, use_cutmix=True, 
                 mixup_alpha=0.2, cutmix_alpha=1.0):
        self.X = X
        self.y = y
        self.batch_size = batch_size
        self.use_mixup = use_mixup
        self.use_cutmix = use_cutmix
        self.mixup_alpha = mixup_alpha
        self.cutmix_alpha = cutmix_alpha
        self.indices = np.arange(len(X))
        
    def __len__(self):
        return len(self.X) // self.batch_size
    
    def __getitem__(self, idx):
        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]
        
        # Convertir en numpy si n√©cessaire
        if hasattr(self.X, 'numpy'):
            X_batch = self.X[batch_indices].numpy().copy()
            y_batch = self.y[batch_indices].numpy().copy()
        else:
            X_batch = self.X[batch_indices].copy()
            y_batch = self.y[batch_indices].copy()
        
        # Apply augmentation randomly (pas en m√™me temps)
        rand_val = np.random.rand()
        if self.use_mixup and rand_val < 0.3:
            X_batch, y_batch = mixup(X_batch, y_batch, self.mixup_alpha)
        elif self.use_cutmix and rand_val < 0.6:
            X_batch, y_batch = cutmix(X_batch, y_batch, self.cutmix_alpha)
            
        return X_batch.astype(np.float32), y_batch.astype(np.float32)
    
    def on_epoch_end(self):
        np.random.shuffle(self.indices)


def create_warmup_scheduler(initial_lr, warmup_epochs=5):
    """Cr√©e un scheduler avec warmup progressif"""
    def scheduler(epoch, lr):
        if epoch < warmup_epochs:
            # Warmup: augmentation progressive du LR
            return initial_lr * (epoch + 1) / warmup_epochs
        return lr
    return LearningRateScheduler(scheduler, verbose=1)


def create_cosine_annealing_scheduler(initial_lr, total_epochs, warmup_epochs=5):
    """Cosine Annealing avec warmup"""
    def scheduler(epoch, lr):
        if epoch < warmup_epochs:
            return initial_lr * (epoch + 1) / warmup_epochs
        else:
            progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)
            return initial_lr * 0.5 * (1 + np.cos(np.pi * progress))
    return LearningRateScheduler(scheduler, verbose=1)


class StochasticWeightAveraging(Callback):
    """SWA - Moyenne des poids pour meilleure g√©n√©ralisation"""
    def __init__(self, start_epoch=30, swa_freq=5):
        super().__init__()
        self.start_epoch = start_epoch
        self.swa_freq = swa_freq
        self.swa_weights = None
        self.swa_count = 0
        
    def on_epoch_end(self, epoch, logs=None):
        if epoch >= self.start_epoch and (epoch - self.start_epoch) % self.swa_freq == 0:
            if self.swa_weights is None:
                self.swa_weights = [w.copy() for w in self.model.get_weights()]
                self.swa_count = 1
            else:
                for i, w in enumerate(self.model.get_weights()):
                    self.swa_weights[i] = (self.swa_weights[i] * self.swa_count + w) / (self.swa_count + 1)
                self.swa_count += 1
            print(f"\nüìä SWA: Moyenne mise √† jour ({self.swa_count} mod√®les)")
    
    def on_train_end(self, logs=None):
        if self.swa_weights is not None:
            print(f"\n‚úÖ Application des poids SWA (moyenne de {self.swa_count} mod√®les)")
            self.model.set_weights(self.swa_weights)


class GradientAccumulation(Callback):
    """Accumulation de gradients pour simuler des batch plus grands"""
    def __init__(self, accumulation_steps=4):
        super().__init__()
        self.accumulation_steps = accumulation_steps
        self.accumulated_gradients = None
        self.step = 0
        
    def on_train_begin(self, logs=None):
        self.accumulated_gradients = [tf.Variable(tf.zeros_like(w), trainable=False) 
                                     for w in self.model.trainable_weights]
    
    def on_batch_end(self, batch, logs=None):
        # Cette impl√©mentation est simplifi√©e - n√©cessite tf.GradientTape dans le training loop
        pass


def curriculum_learning_schedule(epoch, max_epochs):
    """Curriculum learning: commencer simple, finir complexe pour petit dataset"""
    if epoch < max_epochs * 0.3:
        return 0.3  # Peu d'augmentation au d√©but (focus sur apprentissage de base)
    elif epoch < max_epochs * 0.7:
        return 0.7  # Augmentation moyenne (consolidation)
    else:
        return 1.0  # Augmentation maximale (g√©n√©ralisation)


class CurriculumLearningCallback(Callback):
    """Curriculum learning pour petit dataset - augmentation progressive"""
    def __init__(self, max_epochs, augmentation_generator=None):
        super().__init__()
        self.max_epochs = max_epochs
        self.augmentation_generator = augmentation_generator
        self.current_intensity = 0.3
        
    def on_epoch_begin(self, epoch, logs=None):
        # Ajuster l'intensit√© d'augmentation selon le curriculum
        self.current_intensity = curriculum_learning_schedule(epoch, self.max_epochs)
        
        # Si on a un g√©n√©rateur d'augmentation, ajuster ses param√®tres
        if self.augmentation_generator is not None:
            # Ici on pourrait ajuster les param√®tres du g√©n√©rateur
            # Par exemple: rotation_range, zoom_range, etc.
            pass
            
        print(f"üìö Curriculum Learning - √âpoque {epoch+1}/{self.max_epochs}: Intensit√© {self.current_intensity:.1f}")


def progressive_unfreeze_training(model, X_train, y_train, X_val, y_val, model_name="pose_model", 
                                 model_dir=None, use_advanced_aug=True, use_swa=True, 
                                 use_mixed_precision=False, use_gradient_clip=True):
    """
    Entra√Ænement en 3 phases avec techniques avanc√©es
    
    Phase 1: T√™te seule (backbone gel√©) - 20 epochs
    Phase 2: Derni√®res couches du backbone - 20 epochs  
    Phase 3: Tout le backbone - 30 epochs
    
    Techniques avanc√©es:
    - Mixup & CutMix augmentation
    - Cosine Annealing avec warmup
    - Stochastic Weight Averaging (SWA)
    - Mixed Precision Training (FP16)
    - Gradient Clipping
    - Label Smoothing
    """
    print("=" * 60)
    print("üöÄ ENTRA√éNEMENT PROGRESSIF ULTRA-OPTIMIS√â")
    print("=" * 60)
    
    # Activer Mixed Precision Training
    if use_mixed_precision:
        policy = tf.keras.mixed_precision.Policy('mixed_float16')
        tf.keras.mixed_precision.set_global_policy(policy)
        print("‚úÖ Mixed Precision (FP16) activ√©")
    
    # Cr√©er les callbacks de base
    callbacks_base = create_callbacks(model_name, model_dir)
    
    # Ajouter SWA si demand√©
    if use_swa:
        swa_callback = StochasticWeightAveraging(start_epoch=15, swa_freq=3)
        print("‚úÖ Stochastic Weight Averaging (SWA) activ√©")
    
    # ========== PHASE 1: T√™te seule ==========
    print("\n" + "=" * 60)
    print("üìç PHASE 1: Entra√Ænement de la t√™te uniquement")
    print("=" * 60)
    
    # Geler tout le backbone
    for layer in model.layers:
        if 'mobilenet' in layer.name.lower() or 'efficientnet' in layer.name.lower():
            layer.trainable = False
    
    # Utiliser learning rate sp√©cialis√© pour Phase 1
    phase1_lr = config.PHASE_LEARNING_RATES["phase1"]
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=phase1_lr),
        loss='mse',
        metrics=['mae']
    )
    print(f"üéØ Learning Rate Phase 1: {phase1_lr}")
    
    # Warmup scheduler et curriculum learning
    phase1_epochs = config.PHASE_EPOCHS["phase1"]
    warmup_callback = create_warmup_scheduler(phase1_lr, warmup_epochs=8)  # Warmup plus long
    curriculum_callback = CurriculumLearningCallback(max_epochs=phase1_epochs)
    callbacks_phase1 = callbacks_base + [warmup_callback, curriculum_callback]
    
    # Entra√Ænement avec augmentation (epochs augment√©s gr√¢ce √† la r√©gularisation)
    augmentation = create_data_augmentation()
    if augmentation:
        train_gen = augmentation.flow(X_train, y_train, batch_size=config.BATCH_SIZE)
        history1 = model.fit(
            train_gen,
            validation_data=(X_val, y_val),
            epochs=phase1_epochs,
            callbacks=callbacks_phase1,
            verbose=1,
            steps_per_epoch=len(X_train) // config.BATCH_SIZE
        )
    else:
        history1 = model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            batch_size=config.BATCH_SIZE,
            epochs=phase1_epochs,
            callbacks=callbacks_phase1,
            verbose=1
        )
    
    print("\n‚úÖ Phase 1 termin√©e")
    metrics1 = evaluate_model(model, X_val, y_val)
    
    # ========== PHASE 2: D√©gel partiel ==========
    print("\n" + "=" * 60)
    print("üìç PHASE 2: Fine-tuning des derni√®res couches du backbone")
    print("=" * 60)
    
    # D√©geler les 30 derni√®res couches du backbone
    backbone_layer = None
    for layer in model.layers:
        if 'mobilenet' in layer.name.lower() or 'efficientnet' in layer.name.lower():
            backbone_layer = layer
            break
    
    if backbone_layer:
        # D√©geler les derni√®res couches
        for layer in backbone_layer.layers[-30:]:
            layer.trainable = True
        
        print(f"üîì D√©gel√© les 30 derni√®res couches du backbone")
    
    # Optimizer avec learning rate sp√©cialis√© pour Phase 2
    phase2_lr = config.PHASE_LEARNING_RATES["phase2"]
    optimizer2 = keras.optimizers.Adam(
        learning_rate=phase2_lr,
        clipnorm=0.5 if use_gradient_clip else None
    )
    
    model.compile(
        optimizer=optimizer2,
        loss='mse',
        metrics=['mae']
    )
    print(f"üéØ Learning Rate Phase 2: {phase2_lr}")
    
    # Cosine Annealing avec warmup plus long
    phase2_epochs = config.PHASE_EPOCHS["phase2"]
    scheduler2 = create_cosine_annealing_scheduler(phase2_lr, total_epochs=phase2_epochs, warmup_epochs=5)
    curriculum_callback2 = CurriculumLearningCallback(max_epochs=phase2_epochs)
    callbacks_phase2 = callbacks_base + [scheduler2, curriculum_callback2]
    if use_swa:
        callbacks_phase2 = callbacks_phase2 + [swa_callback]
    
    # Phase 2 : utiliser augmentation standard (g√©n√©rateur Keras) au lieu du custom
    print("‚ö†Ô∏è  Utilisation de l'augmentation standard en Phase 2 (√©vite bugs tensors)")
    augmentation = create_data_augmentation()
    if augmentation:
        train_gen = augmentation.flow(X_train, y_train, batch_size=config.BATCH_SIZE)
        history2 = model.fit(
            train_gen,
            validation_data=(X_val, y_val),
            epochs=phase2_epochs,
            callbacks=callbacks_phase2,
            verbose=1,
            steps_per_epoch=len(X_train) // config.BATCH_SIZE
        )
    else:
        history2 = model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            batch_size=config.BATCH_SIZE,
            epochs=phase2_epochs,
            callbacks=callbacks_phase2,
            verbose=1
        )
    
    print("\n‚úÖ Phase 2 termin√©e")
    metrics2 = evaluate_model(model, X_val, y_val)
    
    # ========== PHASE 3: Fine-tuning complet ==========
    print("\n" + "=" * 60)
    print("üìç PHASE 3: Fine-tuning ultra-fin de tout le mod√®le")
    print("=" * 60)
    
    # D√©geler tout le backbone avec learning rates diff√©renci√©s
    if backbone_layer:
        for layer in backbone_layer.layers:
            layer.trainable = True
        print(f"üîì Backbone compl√®tement d√©gel√©")
    
    # Learning rate ultra-fin sp√©cialis√© pour Phase 3
    phase3_lr = config.PHASE_LEARNING_RATES["phase3"]
    optimizer3 = keras.optimizers.AdamW(
        learning_rate=phase3_lr,
        weight_decay=0.0001,
        clipnorm=0.3 if use_gradient_clip else None
    )
    
    model.compile(
        optimizer=optimizer3,
        loss='mse',
        metrics=['mae']
    )
    print(f"üéØ Learning Rate Phase 3: {phase3_lr}")
    
    # Cosine Annealing avec minimum LR
    phase3_epochs = config.PHASE_EPOCHS["phase3"]
    scheduler3 = create_cosine_annealing_scheduler(phase3_lr, total_epochs=phase3_epochs, warmup_epochs=2)
    curriculum_callback3 = CurriculumLearningCallback(max_epochs=phase3_epochs)
    callbacks_phase3 = callbacks_base + [scheduler3, curriculum_callback3]
    if use_swa:
        callbacks_phase3 = callbacks_phase3 + [swa_callback]
    
    # Phase 3 : entra√Ænement simple sans augmentation avanc√©e (plus stable)
    print("‚ö†Ô∏è  Augmentation d√©sactiv√©e en Phase 3 pour stabilit√©")
    history3 = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        batch_size=config.BATCH_SIZE,
        epochs=phase3_epochs,
        callbacks=callbacks_phase3,
        verbose=1
    )
    
    print("\n‚úÖ Phase 3 termin√©e")
    metrics3 = evaluate_model(model, X_val, y_val)
    
    # ========== R√âSUM√â ==========
    print("\n" + "=" * 60)
    print("üìä R√âSUM√â DE L'ENTRA√éNEMENT ULTRA-OPTIMIS√â")
    print("=" * 60)
    
    if use_advanced_aug:
        print("\n‚úÖ Techniques appliqu√©es:")
        print("   - Mixup & CutMix Augmentation")
        print("   - Cosine Annealing avec Warmup")
        if use_swa:
            print(f"   - Stochastic Weight Averaging (SWA)")
        if use_mixed_precision:
            print("   - Mixed Precision Training (FP16)")
        if use_gradient_clip:
            print("   - Gradient Clipping")
    
    print(f"\nüìà Phase 1 (T√™te seule - {config.PHASE_EPOCHS['phase1']} epochs):")
    print(f"   - Loss: {metrics1['loss']:.6f}")
    print(f"   - MAE: {metrics1['mae']:.6f}")
    
    print(f"\nüìà Phase 2 (D√©gel partiel - {config.PHASE_EPOCHS['phase2']} epochs):")
    print(f"   - Loss: {metrics2['loss']:.6f}")
    print(f"   - MAE: {metrics2['mae']:.6f}")
    print(f"   - Am√©lioration: {((metrics1['mae'] - metrics2['mae']) / metrics1['mae'] * 100):.1f}%")
    
    print(f"\nüìà Phase 3 (Fine-tuning complet - {config.PHASE_EPOCHS['phase3']} epochs):")
    print(f"   - Loss: {metrics3['loss']:.6f}")
    print(f"   - MAE: {metrics3['mae']:.6f}")
    print(f"   - Am√©lioration totale: {((metrics1['mae'] - metrics3['mae']) / metrics1['mae'] * 100):.1f}%")
    
    total_epochs = config.PHASE_EPOCHS['phase1'] + config.PHASE_EPOCHS['phase2'] + config.PHASE_EPOCHS['phase3']
    print(f"\nüéØ Total epochs: {total_epochs} ({config.PHASE_EPOCHS['phase1']}+{config.PHASE_EPOCHS['phase2']}+{config.PHASE_EPOCHS['phase3']})")
    print(f"üéØ MAE finale: {metrics3['mae']:.6f} pixels")
    print(f"üéØ Gain vs Phase 1: {((metrics1['mae'] - metrics3['mae']) / metrics1['mae'] * 100):.1f}%")
    
    # Combiner les historiques
    combined_history = {
        'loss': history1.history['loss'] + history2.history['loss'] + history3.history['loss'],
        'val_loss': history1.history['val_loss'] + history2.history['val_loss'] + history3.history['val_loss'],
        'mae': history1.history['mae'] + history2.history['mae'] + history3.history['mae'],
        'val_mae': history1.history['val_mae'] + history2.history['val_mae'] + history3.history['val_mae']
    }
    
    # Cr√©er un objet History-like pour plot_training_history
    class CombinedHistory:
        def __init__(self, history_dict):
            self.history = history_dict
    
    combined_hist = CombinedHistory(combined_history)
    
    return combined_hist, metrics3


if __name__ == "__main__":
    print("=" * 60)
    print("‚úÖ Module advanced_training.py charg√©")
    print("=")
    print("\nüöÄ TECHNIQUES AVANC√âES DISPONIBLES:")
    print("   ‚úì Mixup & CutMix Augmentation")
    print("   ‚úì Cosine Annealing avec Warmup")
    print("   ‚úì Stochastic Weight Averaging (SWA)")
    print("   ‚úì Gradient Clipping Adaptatif")
    print("   ‚úì AdamW avec Weight Decay")
    print("   ‚úì Entra√Ænement progressif 3 phases (70 epochs - warmup optimis√©)")
    print("\nüìù Utilisez main.py avec l'option --advanced-training")
    print("=" * 60)
