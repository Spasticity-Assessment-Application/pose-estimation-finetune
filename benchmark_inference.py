"""
Benchmark de vitesse d'infÃ©rence pour modÃ¨les TFLite
Mesure le temps moyen par frame sur une vidÃ©o
"""
import cv2
import numpy as np
import tensorflow as tf
import argparse
import time
import json
from pathlib import Path


def load_tflite_model(model_path):
    """Charge le modÃ¨le TFLite et dÃ©tecte la taille d'entrÃ©e"""
    interpreter = tf.lite.Interpreter(model_path=model_path)
    interpreter.allocate_tensors()
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    
    # DÃ©tecter la taille d'entrÃ©e
    input_shape = input_details[0]['shape']
    input_size = (input_shape[1], input_shape[2])  # (H, W)
    
    print(f"ğŸ“ Taille d'entrÃ©e dÃ©tectÃ©e: {input_size[0]}Ã—{input_size[1]}")
    
    return interpreter, input_details, output_details, input_size


def preprocess_frame(frame, input_size=(256, 256)):
    """PrÃ©traite une frame pour le modÃ¨le"""
    frame_resized = cv2.resize(frame, input_size)
    frame_normalized = frame_resized.astype(np.float32) / 255.0
    frame_batch = np.expand_dims(frame_normalized, axis=0)
    return frame_batch


def benchmark_inference(model_path, video_path, num_frames=100):
    """
    Mesure la vitesse d'infÃ©rence moyenne

    Args:
        model_path: Chemin vers le modÃ¨le .tflite
        video_path: Chemin vers la vidÃ©o de test
        num_frames: Nombre de frames Ã  tester

    Returns:
        dict: RÃ©sultats du benchmark
    """
    print("=" * 60)
    print("ğŸš€ BENCHMARK VITESSE D'INFÃ‰RENCE")
    print("=" * 60)
    print(f"ğŸ“¦ ModÃ¨le: {model_path}")
    print(f"ğŸ¬ VidÃ©o: {video_path}")
    print(f"ğŸ“Š Frames Ã  tester: {num_frames}")

    # Charger le modÃ¨le
    print("\nâ³ Chargement du modÃ¨le...")
    interpreter, input_details, output_details, input_size = load_tflite_model(model_path)
    print("âœ… ModÃ¨le chargÃ©")
    print(f"ğŸ“ Taille d'entrÃ©e: {input_size[0]}Ã—{input_size[1]}")

    # Ouvrir la vidÃ©o
    print("\nâ³ Ouverture de la vidÃ©o...")
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise ValueError(f"Impossible d'ouvrir la vidÃ©o: {video_path}")

    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    print(f"âœ… VidÃ©o ouverte: {total_frames} frames, {fps:.1f} FPS")

    # Collecter les frames Ã  tester
    frames_to_test = []
    frame_count = 0

    print(f"\nâ³ Collecte de {num_frames} frames...")
    while len(frames_to_test) < num_frames and frame_count < total_frames:
        ret, frame = cap.read()
        if not ret:
            break

        # Prendre une frame tous les 10 frames pour variÃ©tÃ©
        if frame_count % 10 == 0:
            frames_to_test.append(frame)

        frame_count += 1

    cap.release()
    print(f"âœ… {len(frames_to_test)} frames collectÃ©es")

    # Benchmark
    print("\nâ³ Benchmark en cours...")
    inference_times = []

    for i, frame in enumerate(frames_to_test):
        # PrÃ©-traitement
        input_tensor = preprocess_frame(frame, input_size)

        # InfÃ©rence
        start_time = time.perf_counter()
        interpreter.set_tensor(input_details[0]['index'], input_tensor)
        interpreter.invoke()
        output_data = interpreter.get_tensor(output_details[0]['index'])
        end_time = time.perf_counter()

        # Mesurer le temps
        inference_time = (end_time - start_time) * 1000  # en ms
        inference_times.append(inference_time)

        if (i + 1) % 20 == 0:
            print(f"   Frame {i+1}/{len(frames_to_test)}: {inference_time:.2f} ms")

    # Statistiques
    inference_times = np.array(inference_times)
    mean_time = np.mean(inference_times)
    std_time = np.std(inference_times)
    min_time = np.min(inference_times)
    max_time = np.max(inference_times)
    fps_inference = 1000 / mean_time

    results = {
        'model_path': str(model_path),
        'video_path': str(video_path),
        'input_size': [int(input_size[0]), int(input_size[1])],
        'num_frames_tested': len(frames_to_test),
        'mean_inference_time_ms': round(float(mean_time), 3),
        'std_inference_time_ms': round(float(std_time), 3),
        'min_inference_time_ms': round(float(min_time), 3),
        'max_inference_time_ms': round(float(max_time), 3),
        'inference_fps': round(float(fps_inference), 2),
        'video_fps': round(float(fps), 2)
    }

    print("\n" + "=" * 60)
    print("ğŸ“Š RÃ‰SULTATS DU BENCHMARK")
    print("=" * 60)
    print(f"â±ï¸  Temps moyen par frame: {results['mean_inference_time_ms']:.2f} ms")
    print(f"ğŸ¯ Ã‰cart-type: {results['std_inference_time_ms']:.2f} ms")
    print(f"âš¡ FPS d'infÃ©rence: {results['inference_fps']:.1f}")
    print(f"ğŸ¬ FPS vidÃ©o: {results['video_fps']:.1f}")
    print(f"ğŸ“ˆ Ratio: {results['inference_fps']/results['video_fps']:.2f}x")

    # Sauvegarder les rÃ©sultats
    results_path = Path(model_path).parent / "benchmark_results.json"
    with open(results_path, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"\nğŸ’¾ RÃ©sultats sauvegardÃ©s: {results_path}")

    return results


def main():
    parser = argparse.ArgumentParser(description="Benchmark vitesse d'infÃ©rence TFLite")
    parser.add_argument('--model', type=str, required=True, help="Chemin vers le modÃ¨le .tflite")
    parser.add_argument('--video', type=str, required=True, help="Chemin vers la vidÃ©o de test")
    parser.add_argument('--frames', type=int, default=100, help="Nombre de frames Ã  tester")

    args = parser.parse_args()

    try:
        results = benchmark_inference(
            model_path=args.model,
            video_path=args.video,
            num_frames=args.frames
        )
        print("\nâœ… Benchmark terminÃ© avec succÃ¨s!")
    except Exception as e:
        print(f"\nâŒ Erreur: {e}")
        raise


if __name__ == "__main__":
    main()