"""
Export du mod√®le au format TensorFlow Lite pour d√©ploiement mobile
"""
import os
import numpy as np
import tensorflow as tf
from tensorflow import keras
import config


def convert_to_tflite(model_path, output_path, quantize=True, representative_dataset=None):
    """
    Convertit un mod√®le Keras en TensorFlow Lite
    
    Args:
        model_path: Chemin vers le mod√®le SavedModel ou .h5
        output_path: Chemin de sortie pour le fichier .tflite
        quantize: Activer la quantization (int8)
        representative_dataset: Dataset repr√©sentatif pour la quantization
    
    Returns:
        tflite_model_size: Taille du mod√®le en Ko
    """
    print("=" * 60)
    print("üì¶ CONVERSION EN TENSORFLOW LITE")
    print("=" * 60)
    
    # Charger le mod√®le
    print(f"\nüìÇ Chargement du mod√®le depuis: {model_path}")
    
    # Cr√©er le converter
    if model_path.endswith('.h5'):
        model = keras.models.load_model(model_path)
        converter = tf.lite.TFLiteConverter.from_keras_model(model)
    else:
        # SavedModel format
        converter = tf.lite.TFLiteConverter.from_saved_model(model_path)
    
    # Configuration du converter
    if quantize and representative_dataset is not None:
        print("\n‚öôÔ∏è  Configuration de la quantization INT8 optimis√©e...")
        
        # Activer la quantization post-training
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        
        # Fournir un dataset repr√©sentatif pour la quantization
        converter.representative_dataset = representative_dataset
        
        # AM√âLIORATION 1: Garder les entr√©es/sorties en float32 pour plus de pr√©cision
        # (seulement les poids internes sont quantiz√©s en INT8)
        converter.target_spec.supported_ops = [
            tf.lite.OpsSet.TFLITE_BUILTINS_INT8,
            tf.lite.OpsSet.TFLITE_BUILTINS  # Fallback pour op√©rations non support√©es
        ]
        # NE PAS quantizer les entr√©es/sorties pour garder la pr√©cision
        # converter.inference_input_type = tf.uint8  # D√âSACTIV√â
        # converter.inference_output_type = tf.uint8  # D√âSACTIV√â
        
    elif quantize:
        # Quantization simple sans dataset repr√©sentatif (float16)
        print("\n‚öôÔ∏è  Configuration de la quantization FLOAT16...")
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        converter.target_spec.supported_types = [tf.float16]
    else:
        print("\n‚öôÔ∏è  Pas de quantization (mod√®le float32)")
    
    # Convertir
    print("\nüîÑ Conversion en cours...")
    tflite_model = converter.convert()
    
    # Sauvegarder
    with open(output_path, 'wb') as f:
        f.write(tflite_model)
    
    # Afficher la taille
    tflite_model_size = len(tflite_model) / 1024  # en Ko
    print(f"\n‚úÖ Mod√®le TFLite sauvegard√©: {output_path}")
    print(f"üìä Taille du mod√®le: {tflite_model_size:.2f} Ko")
    
    print("=" * 60)
    
    return tflite_model_size


def create_representative_dataset_generator(X_val, num_samples=100):
    """
    Cr√©e un g√©n√©rateur de dataset repr√©sentatif pour la quantization
    AM√âLIOR√â: Utilise plus d'√©chantillons et couvre mieux la distribution
    
    Args:
        X_val: Dataset de validation
        num_samples: Nombre d'√©chantillons √† utiliser (augment√© pour meilleure calibration)
    
    Returns:
        representative_dataset_gen: G√©n√©rateur pour le converter
    """
    def representative_dataset_gen():
        # AM√âLIORATION 2: Utiliser TOUS les √©chantillons de validation pour meilleure calibration
        # Au lieu de prendre s√©quentiellement, on m√©lange pour couvrir toute la distribution
        indices = np.random.permutation(len(X_val))[:num_samples]
        for idx in indices:
            # Prendre un √©chantillon
            sample = X_val[idx:idx+1].astype(np.float32)
            yield [sample]
    
    return representative_dataset_gen


def test_tflite_model(tflite_path, X_test, y_test, num_samples=10):
    """
    Teste le mod√®le TFLite et compare avec les pr√©dictions originales
    
    Args:
        tflite_path: Chemin vers le mod√®le .tflite
        X_test: Images de test
        y_test: Heatmaps de test
        num_samples: Nombre d'√©chantillons √† tester
    
    Returns:
        avg_error: Erreur moyenne
    """
    print("\nüß™ Test du mod√®le TFLite...")
    
    # Charger l'interpr√©teur TFLite
    interpreter = tf.lite.Interpreter(model_path=tflite_path)
    interpreter.allocate_tensors()
    
    # Obtenir les d√©tails des entr√©es/sorties
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    
    print(f"\nüìä D√©tails de l'interpr√©teur:")
    print(f"   - Input shape: {input_details[0]['shape']}")
    print(f"   - Input type: {input_details[0]['dtype']}")
    print(f"   - Output shape: {output_details[0]['shape']}")
    print(f"   - Output type: {output_details[0]['dtype']}")
    
    # Tester sur quelques √©chantillons
    errors = []
    for i in range(min(num_samples, len(X_test))):
        # Pr√©parer l'entr√©e
        input_data = X_test[i:i+1].astype(np.float32)
        
        # Si le mod√®le attend des uint8, il faut quantizer l'entr√©e
        if input_details[0]['dtype'] == np.uint8:
            input_scale, input_zero_point = input_details[0]['quantization']
            input_data = (input_data / input_scale + input_zero_point).astype(np.uint8)
        
        # Inf√©rence
        interpreter.set_tensor(input_details[0]['index'], input_data)
        interpreter.invoke()
        
        # R√©cup√©rer la sortie
        output_data = interpreter.get_tensor(output_details[0]['index'])
        
        # Si la sortie est quantiz√©e, il faut la d√©quantizer
        if output_details[0]['dtype'] == np.uint8:
            output_scale, output_zero_point = output_details[0]['quantization']
            output_data = (output_data.astype(np.float32) - output_zero_point) * output_scale
        
        # Calculer l'erreur
        error = np.mean(np.abs(output_data - y_test[i:i+1]))
        errors.append(error)
    
    avg_error = np.mean(errors)
    print(f"\nüìä R√©sultats du test:")
    print(f"   - Nombre d'√©chantillons test√©s: {len(errors)}")
    print(f"   - Erreur moyenne (MAE): {avg_error:.6f}")
    
    return avg_error


def export_model(model=None, model_path=None, X_val=None, model_name="pose_model"):
    """
    Pipeline complet d'export du mod√®le en TFLite
    
    Args:
        model: Mod√®le Keras (optionnel si model_path est fourni)
        model_path: Chemin vers le mod√®le sauvegard√© (optionnel si model est fourni)
        X_val: Dataset de validation pour la quantization
        model_name: Nom du mod√®le
    
    Returns:
        tflite_path: Chemin vers le fichier .tflite
    """
    print("=" * 60)
    print("üöÄ EXPORT DU MOD√àLE EN TENSORFLOW LITE")
    print("=" * 60)
    
    # Si un mod√®le Keras est fourni, le sauvegarder d'abord
    if model is not None:
        saved_model_dir = os.path.join(config.MODELS_DIR, f"{model_name}_for_export")
        print(f"\nüíæ Sauvegarde du mod√®le au format SavedModel...")
        model.save(saved_model_dir, save_format='tf')
        model_path = saved_model_dir
    
    if model_path is None:
        raise ValueError("Vous devez fournir soit 'model' soit 'model_path'")
    
    # Chemin de sortie pour le .tflite
    tflite_path = os.path.join(config.MODELS_DIR, config.TFLITE_MODEL_NAME)
    
    # Cr√©er le dataset repr√©sentatif si X_val est fourni et quantization activ√©e
    representative_dataset = None
    if config.TFLITE_QUANTIZATION and X_val is not None:
        # AM√âLIORATION 3: Utiliser plus d'√©chantillons pour la calibration (500 au lieu de 100)
        num_calibration_samples = min(500, len(X_val))
        print(f"\nüìä Cr√©ation du dataset repr√©sentatif ({num_calibration_samples} √©chantillons)...")
        representative_dataset = create_representative_dataset_generator(
            X_val, 
            num_samples=num_calibration_samples
        )
    
    # Convertir en TFLite
    tflite_size = convert_to_tflite(
        model_path=model_path,
        output_path=tflite_path,
        quantize=config.TFLITE_QUANTIZATION,
        representative_dataset=representative_dataset
    )
    
    print(f"\n‚úÖ Export termin√©!")
    print(f"üì± Mod√®le pr√™t pour le d√©ploiement mobile: {tflite_path}")
    
    # Instructions pour l'utilisation
    print("\n" + "=" * 60)
    print("üì± UTILISATION DU MOD√àLE TFLITE")
    print("=" * 60)
    print("\nü§ñ Android (Java/Kotlin):")
    print("   1. Ajoutez le .tflite dans assets/")
    print("   2. Ajoutez la d√©pendance: implementation 'org.tensorflow:tensorflow-lite:2.x.x'")
    print("   3. Chargez avec: Interpreter.create(...)")
    print("   4. Utilisez GPU Delegate ou NNAPI pour acc√©l√©rer")
    
    print("\nüçé iOS (Swift/Objective-C):")
    print("   1. Ajoutez le .tflite au projet Xcode")
    print("   2. Ajoutez TensorFlowLiteSwift via CocoaPods/SPM")
    print("   3. Chargez avec: Interpreter(modelPath: ...)")
    print("   4. Utilisez Metal Delegate pour acc√©l√©rer")
    
    print("\nüîÑ Conversion CoreML (optionnel pour iOS):")
    print("   - Utilisez coremltools pour convertir .tflite en .mlmodel")
    print("=" * 60)
    
    return tflite_path


if __name__ == "__main__":
    print("‚úÖ Module export_tflite.py charg√© avec succ√®s")
    print("üìù Utilisez main.py pour exporter le mod√®le apr√®s l'entra√Ænement")
