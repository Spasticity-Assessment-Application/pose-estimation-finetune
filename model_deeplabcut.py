"""
Architecture de modÃ¨le inspirÃ©e de DeepLabCut
- TÃªte simplifiÃ©e avec upsampling bilinear
- Conv 1x1 pour rÃ©duction de dimensionnalitÃ©
- Activation linÃ©aire (pas de sigmoid)
- OptimisÃ© pour petit dataset
"""
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Model
from tensorflow.keras.applications import (
    MobileNetV2, MobileNetV3Small, MobileNetV3Large,
    EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3, EfficientNetB4
)
import config


def get_backbone_deeplabcut(backbone_name="MobileNetV3Small", input_shape=(256, 256, 3)):
    """
    Charge le backbone avec configuration DeepLabCut-friendly
    
    Args:
        backbone_name: Nom du backbone
        input_shape: Forme de l'entrÃ©e (H, W, C)
    
    Returns:
        backbone: ModÃ¨le Keras du backbone
    """
    if backbone_name == "MobileNetV2":
        backbone = MobileNetV2(
            input_shape=input_shape,
            include_top=False,
            weights='imagenet',
            alpha=1.0
        )
    elif backbone_name == "MobileNetV3Small":
        backbone = MobileNetV3Small(
            input_shape=input_shape,
            include_top=False,
            weights='imagenet',
            alpha=1.0,
            minimalistic=True,  # âœ… Utilise ReLU au lieu de hard-swish
            include_preprocessing=False
        )
    elif backbone_name == "MobileNetV3Large":
        backbone = MobileNetV3Large(
            input_shape=input_shape,
            include_top=False,
            weights='imagenet',
            alpha=1.0,
            minimalistic=True,
            include_preprocessing=False
        )
    elif backbone_name == "EfficientNetLite0":
        backbone = EfficientNetB0(
            input_shape=input_shape,
            include_top=False,
            weights=config.PRETRAINED_WEIGHTS
        )
    elif backbone_name == "EfficientNetLite1":
        backbone = EfficientNetB1(
            input_shape=input_shape,
            include_top=False,
            weights=config.PRETRAINED_WEIGHTS
        )
    elif backbone_name == "EfficientNetLite2":
        backbone = EfficientNetB2(
            input_shape=input_shape,
            include_top=False,
            weights=config.PRETRAINED_WEIGHTS
        )
    elif backbone_name == "EfficientNetLite3":
        backbone = EfficientNetB3(
            input_shape=input_shape,
            include_top=False,
            weights=config.PRETRAINED_WEIGHTS
        )
    elif backbone_name == "EfficientNetLite4":
        backbone = EfficientNetB4(
            input_shape=input_shape,
            include_top=False,
            weights=config.PRETRAINED_WEIGHTS
        )
    else:
        raise ValueError(f"Backbone {backbone_name} non supportÃ© pour DeepLabCut mode")
    
    return backbone


def build_deeplabcut_model(num_keypoints=3, backbone_name="MobileNetV3Small", 
                           input_shape=(256, 256, 3), heatmap_stride=4):
    """
    Construit un modÃ¨le de pose estimation style DeepLabCut
    
    Architecture:
        1. Backbone prÃ©-entraÃ®nÃ© (stride 32)
        2. Conv 1Ã—1 pour rÃ©duire les channels
        3. Upsampling bilinear progressif
        4. Conv 1Ã—1 finale pour gÃ©nÃ©rer les heatmaps
        5. Activation linÃ©aire (pas de sigmoid)
    
    Args:
        num_keypoints: Nombre de points clÃ©s
        backbone_name: Nom du backbone
        input_shape: Forme de l'entrÃ©e (H, W, C)
        heatmap_stride: Stride final des heatmaps (4 = haute rÃ©solution, 8 = moyenne)
    
    Returns:
        model: ModÃ¨le Keras
    """
    print("=" * 60)
    print("ğŸ”¬ CONSTRUCTION DU MODÃˆLE DEEPLABCUT-STYLE")
    print("=" * 60)
    print(f"ğŸ“¦ Backbone: {backbone_name}")
    print(f"ğŸ“Š Input shape: {input_shape}")
    print(f"ğŸ“Š Heatmap stride: {heatmap_stride} (rÃ©solution: {input_shape[0]//heatmap_stride}Ã—{input_shape[1]//heatmap_stride})")
    
    # 1. EntrÃ©e
    inputs = keras.Input(shape=input_shape, name="image_input")
    
    # 2. Backbone
    backbone = get_backbone_deeplabcut(backbone_name, input_shape)
    backbone.trainable = False  # GelÃ© au dÃ©part
    x = backbone(inputs)
    
    # Afficher la shape de sortie du backbone
    print(f"ğŸ“ Backbone output shape: {x.shape}")
    
    # 3. RÃ©duction de dimensionnalitÃ© (Conv 1Ã—1)
    # RÃ©duit le nombre de channels pour accÃ©lÃ©rer l'upsampling
    x = layers.Conv2D(256, (1, 1), padding='same', name='reduce_channels')(x)
    x = layers.BatchNormalization(name='bn_reduce')(x)
    x = layers.ReLU(name='relu_reduce')(x)
    print(f"ğŸ“ After channel reduction: 256 channels")
    
    # 4. Upsampling progressif (bilinear interpolation)
    # Le backbone a un stride de 32, on veut arriver Ã  stride 4 ou 8
    # 32 â†’ 16 â†’ 8 â†’ 4 (3 Ã©tapes d'upsampling 2Ã—)
    
    current_stride = 32
    target_stride = heatmap_stride
    
    upsample_step = 1
    while current_stride > target_stride:
        # Upsampling bilinear 2Ã—
        x = layers.UpSampling2D(size=(2, 2), interpolation='bilinear', 
                                name=f'upsample_{upsample_step}')(x)
        
        # Conv pour affiner aprÃ¨s upsampling
        filters = max(128 // upsample_step, 64)  # RÃ©duire progressivement les filters
        x = layers.Conv2D(filters, (3, 3), padding='same', 
                         name=f'refine_{upsample_step}')(x)
        x = layers.BatchNormalization(name=f'bn_refine_{upsample_step}')(x)
        x = layers.ReLU(name=f'relu_refine_{upsample_step}')(x)
        
        current_stride //= 2
        upsample_step += 1
        print(f"ğŸ“ After upsample step {upsample_step-1}: stride={current_stride}, filters={filters}")
    
    # 5. TÃªte finale: Conv 1Ã—1 pour gÃ©nÃ©rer les heatmaps
    heatmaps = layers.Conv2D(
        num_keypoints, 
        (1, 1), 
        padding='same',
        activation='linear',  # âœ… Activation linÃ©aire comme DeepLabCut
        name='heatmaps_output'
    )(x)
    
    print(f"ğŸ“ Final heatmaps shape: {heatmaps.shape}")
    
    # 6. Construire le modÃ¨le
    model = Model(inputs=inputs, outputs=heatmaps, name=f"DeepLabCut_{backbone_name}")
    
    # RÃ©sumÃ©
    print(f"\nğŸ“Š RÃ©sumÃ© du modÃ¨le:")
    print(f"   - ParamÃ¨tres totaux: {model.count_params():,}")
    trainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])
    print(f"   - ParamÃ¨tres entraÃ®nables: {trainable_params:,}")
    print(f"   - ParamÃ¨tres gelÃ©s: {model.count_params() - trainable_params:,}")
    print("=" * 60)
    
    return model


def create_deeplabcut_model():
    """
    Factory pour crÃ©er le modÃ¨le DeepLabCut avec la config globale
    """
    return build_deeplabcut_model(
        num_keypoints=config.NUM_KEYPOINTS,
        backbone_name=config.BACKBONE,
        input_shape=config.INPUT_SHAPE,
        heatmap_stride=config.DEEPLABCUT_HEATMAP_STRIDE
    )


if __name__ == "__main__":
    print("âœ… Module model_deeplabcut.py chargÃ©")
    print("ğŸ“ Utilisez create_deeplabcut_model() pour crÃ©er le modÃ¨le")
